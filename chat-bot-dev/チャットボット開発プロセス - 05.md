---
date: 2025-07-21
tags:
  - 営業
  - API
Gemini: https://g.co/gemini/share/33ba7deb7ae8
release: https://g.co/gemini/share/0b37e90a6003
---
## チャットボットの構築

次の手順は、このベクトルデータベースを活用して、実際にGeminiモデルと連携させ、質問応答システム（チャットボット）を構築することです。

具体的には、以下のステップに進みます。

### 次のステップ：RAGパイプラインの構築と質問応答

1. **ベクトルデータベースのロード:**
    
    - データベースが既に構築されているので、毎回再構築する必要はありません。スクリプト実行時に、既存のデータベースをロードして利用します。
        
2. **Geminiモデルの初期化:**
    
    - 質問に答えるための大規模言語モデル（LLM）であるGemini 1.5 Flashを初期化します。
        
3. **RAGチェーン（またはパイプライン）の構築:**
    
    - LangChainなどのフレームワークを使って、以下の流れを自動化する「チェーン」を構築します。
        
        - ユーザーからの質問を受け取る。
            
        - 質問を埋め込みベクトルに変換する。
            
        - ベクトルデータベースで、質問のベクトルに最も類似する資料のチャンク（関連ドキュメント）を検索する。
            
        - 検索された関連ドキュメントと元の質問を組み合わせて、Geminiモデルへのプロンプトを作成する。
            
        - Geminiモデルにプロンプトを渡し、回答を生成させる。
            
        - 生成された回答をユーザーに返す。
            
4. **質問応答のテスト:**
    
    - 実際に質問を投げかけて、期待通りの回答が得られるか、またその回答が取得した資料に基づいているかを確認します。
        

### 具体的なコード例（前回の続き）

前回のスクリプトの最後に、テスト検索の部分がありましたが、それをさらに拡張してRAGパイプラインを組み込みます。

Python

```
# your_project_root/your_script.py (または新しいファイル例: chat_bot.py)

import os
from dotenv import load_dotenv
from langchain_community.document_loaders import DirectoryLoader, UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate

# 1. 環境変数のロード
load_dotenv()
google_api_key = os.getenv("GOOGLE_API_KEY")

if not google_api_key:
    raise ValueError("GOOGLE_API_KEY not found in .env file or environment variables.")

# 2. 資料のパスとベクトルデータベースの保存先パスを定義
data_path = "./data/"
vector_db_path = "./vector_db_data"

# ★★★ データベースが既に構築されている前提で、ロードから開始 ★★★
# 埋め込みモデルを再度初期化する必要がある
embeddings_model = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=google_api_key)

print(f"Loading vector database from {vector_db_path}...")
try:
    # 既存のChromaDBデータベースをロード
    vectorstore = Chroma(persist_directory=vector_db_path, embedding_function=embeddings_model)
    print("Vector database loaded successfully.")
except Exception as e:
    print(f"Error loading vector database: {e}")
    print("Please ensure the database was built correctly and the 'vector_db_data' directory exists.")
    exit()

# 3. Gemini 1.5 Flash モデルの初期化
# temperature: 創造性（0.0が最も確定的、1.0が最も創造的）
llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0.3, google_api_key=google_api_key)
print("Gemini 1.5 Flash model initialized.")

# 4. RAGチェーンの構築
# Retriever: ベクトルデータベースから関連ドキュメントを取得するコンポーネント
retriever = vectorstore.as_retriever(search_kwargs={"k": 3}) # 検索する関連ドキュメントの数を指定 (例: 上位3件)

# プロンプトテンプレートの定義（オプションだが推奨）
# LLMに渡すプロンプトの形式を定義します。
# {context} には検索された関連ドキュメントが、{question} にはユーザーの質問が入ります。
prompt_template = """あなたは与えられた資料に基づいて質問に答えるAIアシスタントです。
以下の「コンテキスト」に記載されている情報のみを使用して、質問に正確かつ簡潔に答えてください。
もしコンテキストに質問の答えが見つからない場合は、「提供された情報からは答えを見つけることができませんでした。」と答えてください。
不確かな情報やコンテキストにない情報を追加しないでください。

コンテキスト:
{context}

質問: {question}

回答:
"""
# PromptTemplateオブジェクトを作成
PROMPT = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
)

# RetrievalQA チェーンを作成
# chain_type="stuff": 検索された全てのドキュメントを一つの大きなプロンプトに詰め込んでLLMに渡す
# return_source_documents=True: LLMの回答だけでなく、その根拠となったソースドキュメントも結果に含める
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True,
    chain_type_kwargs={"prompt": PROMPT} # カスタムプロンプトを使用する場合
)

print("RAG chain constructed.")

# 5. 質問応答のループ
print("\n--- チャットボットを開始します (終了するには 'exit' または 'quit' と入力してください) ---")
while True:
    user_query = input("\nあなたの質問: ")
    if user_query.lower() in ["exit", "quit"]:
        print("チャットボットを終了します。")
        break

    print("回答を生成中...")
    try:
        result = qa_chain.invoke({"query": user_query})

        print("\n--- AIの回答 ---")
        print(result["result"])

        print("\n--- 参照元ドキュメント ---")
        if result.get("source_documents"):
            for i, doc in enumerate(result["source_documents"]):
                print(f"  ドキュメント {i+1}: {doc.metadata.get('source', '不明なソース')}")
                # print(f"    内容の冒頭: {doc.page_content[:150]}...") # 必要に応じて内容の一部を表示
        else:
            print("  参照元ドキュメントはありませんでした。")

    except Exception as e:
        print(f"エラーが発生しました: {e}")
        print("APIキーが正しく設定されているか、またはAPIのレート制限に達していないか確認してください。")

```

### この後の改善点と発展

- **エラーハンドリングの強化:** API呼び出しの失敗や、関連ドキュメントが見つからない場合の処理をより堅牢にする。
    
- **プロンプトエンジニアリング:** `prompt_template` を調整して、より良い回答が得られるように工夫する。
    
- **チャンク戦略の最適化:** `RecursiveCharacterTextSplitter` の`chunk_size`や`chunk_overlap`を調整したり、他の`TextSplitter`を試したりして、RAGの精度を向上させる。
    
- **Retrieverのチューニング:** `retriever` の `search_kwargs={"k": N}` の `N` の値を変更して、取得するドキュメント数を調整する。また、より高度な検索方法（MMR検索など）を検討する。
    
- **ユーザーインターフェース:** コマンドラインからの入力だけでなく、StreamlitやGradioなどのライブラリを使って簡単なWeb UIを構築する。
    
- **デプロイ:** Google Cloud Runなどのサービスにデプロイして、Webアプリケーションとして公開する。
    

これで、NotebookLMからエクスポートした資料を基にした、Gemini 1.5 Flash搭載のチャットボットの基本的な骨格が完成します。ぜひ、色々な質問を試してみてください！

---
## **技術的キーワード一覧**
### **🧠 LLM & Embedding**

|**キーワード**|**概要**|
|---|---|
|**Gemini 1.5 Flash**|Google LLM。質問応答と埋め込み生成を担う。|
|**ChatGoogleGenerativeAI**|LangChain のクラス。Gemini をチャット形式で呼び出すラッパー。|
|**GoogleGenerativeAIEmbeddings**|LangChain で Gemini の埋め込みモデルを利用するためのクラス。|

---

### **🔍 ベクトルデータベース & 検索**

|**キーワード**|**概要**|
|---|---|
|**ベクトルデータベースのロード**|既存インデックスをディスクから読み込み、再構築コストを回避。|
|**Chroma**|ローカルで扱いやすい OSS ベクトル DB。persist_directory で永続化。|
|**Retriever**|vectorstore.as_retriever() で生成。k-NN 検索で関連チャンクを取得。|
|**MMR検索**|Maximal Marginal Relevance。多様性を考慮したドキュメント検索手法。|

---

### **🛠️ RAG チェーン**

|**キーワード**|**概要**|
|---|---|
|**RAGチェーン / パイプライン**|検索→プロンプト生成→LLM への投入→回答生成を自動化する処理フロー。|
|**RetrievalQA**|LangChain の QA 用チェーン。検索結果を LLM に渡し根拠付き回答を返す。|
|**PromptTemplate**|プロンプト文を雛形化し、{context} と {question} を差し込むテンプレート。|
|**chain_type=“stuff”**|検索文書をそのまま連結して LLM に渡すチェーン方式。|

---

### **📚 テキスト前処理**

|**キーワード**|**概要**|
|---|---|
|**RecursiveCharacterTextSplitter**|長文を chunk_size と chunk_overlap 指定で分割する LangChain ユーティリティ。|
|**チャンク戦略最適化**|スプリッタ設定を調整し、検索精度とコンテキスト制限のバランスを取る手法。|

---

### **🔐 環境設定 & 認証**

|**キーワード**|**概要**|
|---|---|
|**dotenv / load_dotenv**|.env から API キーを読み込む Python ライブラリ。|
|**GOOGLE_API_KEY**|Gemini API 認証に使用する環境変数。|

---

### **💻 UI & デプロイ**

|**キーワード**|**概要**|
|---|---|
|**Streamlit / Gradio**|Python だけで簡易 Web チャット UI を構築できるライブラリ。|
|**Google Cloud Run**|コンテナを自動スケールで実行し、チャットボットを公開できるマネージドサービス。|

---

### **⚙️ 運用・改善**

|**キーワード**|**概要**|
|---|---|
|**エラーハンドリング強化**|API 失敗時や検索ヒットなし時の例外処理を堅牢化する実装方針。|
|**retriever k 調整**|取得チャンク数を変更し回答品質をチューニングするパラメータ最適化。|