---
date: 2025-07-21
tags:
  - 営業
  - API
Gemini: https://g.co/gemini/share/33ba7deb7ae8
release: https://g.co/gemini/share/0b37e90a6003
---

## ベクトルデータベースの作り方

ベクトルデータベースの構築は、RAG (Retrieval-Augmented Generation) システムの核となる部分です。

### 1. ベクトルデータベースの概要

ベクトルデータベースは、テキスト、画像、音声などのデータから抽出された「ベクトル埋め込み（Vector Embeddings）」を保存し、高速に類似検索（ベクトル検索）を行うための特殊なデータベースです。

**基本的な流れ:**

1. **資料の準備:** NotebookLMからエクスポートした資料（テキストファイル、PDFなど）を準備します。
    
2. **チャンク化 (Chunking):** 長い資料を意味のある小さな塊（チャンク）に分割します。
    
3. **埋め込み (Embedding) の生成:** 各チャンクを、Gemini 1.5 Flashが提供する埋め込みモデル（または別の適切な埋め込みモデル）を使って数値のベクトルに変換します。
    
4. **ベクトルデータベースへの保存:** 生成されたベクトルと、それに対応する元のテキストチャンクをベクトルデータベースに保存します。
    

### 2. 使用するツールとライブラリ

今回はPythonをベースに、以下のツールを想定して説明します。

- **Python:** プログラミング言語
    
- **Google Generative AI SDK:** Geminiの埋め込みモデルを利用するため。
    
- **LangChainまたはLlamaIndex:** 資料のロード、チャンク化、埋め込み、ベクトルデータベースとの連携を効率化するフレームワーク。
    
- **ベクトルデータベースライブラリ（選定例）:**
    
    - **ChromaDB:** 軽量でPythonで簡単にローカルで試せる。
        
    - **FAISS:** Facebook AI提供の高速な類似検索ライブラリ。大規模なデータに適しているが、インメモリまたはディスクに保存する形式。
        
    - **Pinecone, Weaviate, Milvus, Qdrant:** クラウドベースまたは専用サーバーで動く、よりスケーラブルなプロダクション向けベクトルデータベース。
        
- **データローダー:** `unstructured` や `pypdf` など、多様なファイル形式を扱うためのライブラリ。
    

### 3. ベクトルデータベースの具体的な作り方（ChromaDBとLangChainの例）

ChromaDBはローカルで簡単に試せるため、開発初期段階や小規模なプロジェクトに適しています。

#### ステップ1: 環境構築

必要なライブラリをインストールします。

Bash

```
pip install google-generativeai # Gemini APIを使うため
pip install -U langchain # LangChainの最新版
pip install pypdf # PDFファイルを扱う場合
pip install unstructured # 多様なファイル形式を扱う場合
pip install chromadb # ベクトルデータベース
```

Google Cloud Projectの設定とAPIキーの取得が必要です。

1. Google Cloud Consoleで新しいプロジェクトを作成または既存のプロジェクトを選択します。
    
2. 「Vertex AI API」と「Generative Language API」を有効にします。
    
3. サービスアカウントキーまたはAPIキーを作成し、安全な場所に保存します。環境変数に設定するのが一般的です。
    

Python

```
# 環境変数の設定 (例: .env ファイルを使用する場合)
# pip install python-dotenv
# .env ファイルに以下を記述:
# GOOGLE_API_KEY="YOUR_API_KEY"
import os
from dotenv import load_dotenv
load_dotenv()
os.environ["GOOGLE_API_API_KEY"] = os.getenv("GOOGLE_API_KEY") # LangChainが参照する環境変数名

# または直接設定
# os.environ["GOOGLE_API_API_KEY"] = "YOUR_API_KEY"
```

#### ステップ2: 資料の準備とロード

NotebookLMからエクスポートした資料を特定のディレクトリ（例: `data/`）に置きます。

```
data/
├── document1.txt
├── document2.pdf
└── chapter3.docx
```

Pythonコードで資料をロードします。

Python

```
from langchain_community.document_loaders import TextLoader, PyPDFLoader, Docx2txtLoader
from langchain_community.document_loaders import DirectoryLoader

# 資料が格納されているディレクトリ
data_path = "./data/"

# ディレクトリ内の全てのテキストファイルをロード
# loader_txt = DirectoryLoader(data_path, glob="**/*.txt", loader_cls=TextLoader)
# docs_txt = loader_txt.load()

# ディレクトリ内の全てのPDFファイルをロード
# loader_pdf = DirectoryLoader(data_path, glob="**/*.pdf", loader_cls=PyPDFLoader)
# docs_pdf = loader_pdf.load()

# ディレクトリ内の全てのDocxファイルをロード (unstructured を使うとより汎用的)
# loader_docx = DirectoryLoader(data_path, glob="**/*.docx", loader_cls=Docx2txtLoader)
# docs_docx = loader_docx.load()

# 全てのファイルをまとめてロードする例 (unstructuredを使うとより多様なファイルに対応)
# pip install "unstructured[all-docs]" # 全てのドキュメントタイプをサポートする場合
from langchain_community.document_loaders import UnstructuredFileLoader
all_documents = []
for file_name in os.listdir(data_path):
    file_path = os.path.join(data_path, file_name)
    try:
        loader = UnstructuredFileLoader(file_path)
        all_documents.extend(loader.load())
    except Exception as e:
        print(f"Error loading {file_path}: {e}")

print(f"Loaded {len(all_documents)} documents.")
# for doc in all_documents:
#     print(doc.metadata) # ドキュメントのメタデータ（ファイルパスなど）
#     print(doc.page_content[:100]) # ドキュメントの内容の冒頭
```

#### ステップ3: チャンク化 (Splitting)

ロードした長いドキュメントを、LLMのコンテキストウィンドウに収まるような意味のある小さなチャンクに分割します。

Python

```
from langchain.text_splitter import RecursiveCharacterTextSplitter

# テキストスプリッターの設定
# chunk_size: 1チャンクあたりの最大文字数
# chunk_overlap: チャンク間の重複部分（文脈を維持するため）
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
    is_separator_regex=False,
)

# ドキュメントをチャンクに分割
chunks = text_splitter.split_documents(all_documents)

print(f"Split into {len(chunks)} chunks.")
# for i, chunk in enumerate(chunks[:5]):
#     print(f"--- Chunk {i+1} (Length: {len(chunk.page_content)}) ---")
#     print(chunk.page_content)
#     print("\n")
```

#### ステップ4: 埋め込み (Embedding) の生成

Gemini 1.5 Flashの埋め込みモデルを使って、各チャンクをベクトルに変換します。

Python

```
from langchain_google_genai import GoogleGenerativeAIEmbeddings

# Geminiの埋め込みモデルを初期化
# model_name に適切な埋め込みモデルを指定します。
# 2024年7月時点では "models/embedding-001" が一般的です。
embeddings_model = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

# 例: 最初のチャンクの埋め込みを生成
# chunk_embedding = embeddings_model.embed_query(chunks[0].page_content)
# print(f"Embedding dimension: {len(chunk_embedding)}")
```

#### ステップ5: ベクトルデータベースへの保存

ChromaDBを初期化し、生成したチャンクと埋め込みを保存します。

Python

```
from langchain_community.vectorstores import Chroma

# ChromaDBのディレクトリパス (ここにデータが保存される)
chroma_db_path = "./chroma_db"

# ChromaDBを初期化し、チャンクと埋め込みモデルを渡してデータを保存
# これにより、チャンクが自動的にベクトル化され、データベースに保存されます。
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings_model,
    persist_directory=chroma_db_path # データベースをディスクに保存する場合
)

# データベースをディスクに永続化 (既に初期化時に persist_directory を指定していれば自動的に行われる)
# vectorstore.persist() # 明示的に保存を指示する場合

print(f"Vector database created and saved to {chroma_db_path}")

# 永続化されたデータベースをロードする場合
# vectorstore = Chroma(persist_directory=chroma_db_path, embedding_function=embeddings_model)
# print("Vector database loaded from disk.")
```

#### ステップ6: ベクトルデータベースからの検索（テスト）

保存したデータベースに対して、類似検索（質問に対する関連チャンクの検索）を試します。

Python

```
# 質問
query = "ノートブックLMの主要な機能は何ですか？"

# 類似チャンクを検索
# k=指定した数の最も関連性の高いチャンクを取得
relevant_docs = vectorstore.similarity_search(query, k=3)

print(f"\n--- Relevant documents for query: '{query}' ---")
for i, doc in enumerate(relevant_docs):
    print(f"Document {i+1} (Source: {doc.metadata.get('source', 'N/A')}, Page: {doc.metadata.get('page', 'N/A')})")
    print(doc.page_content[:500]) # 関連チャンクの内容を表示
    print("-" * 50)
```

### 4. 次のステップ：RAGパイプラインの構築

ベクトルデータベースが完成したら、いよいよRAGパイプラインを構築し、Gemini 1.5 Flashと連携させます。

Python

```
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA

# Gemini 1.5 Flash モデルを初期化
# model_name に "gemini-1.5-flash-latest" または "gemini-1.5-flash" を指定します。
llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0.7)

# RetrievalQA チェーンを作成
# retriever: ベクトルデータベースから関連ドキュメントを取得する役割
# llm: 取得したドキュメントと質問に基づいて回答を生成する役割
# return_source_documents=True: 回答の根拠となったドキュメントも返すように設定
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff", # 取得したドキュメントをそのままLLMに詰める（stuff）
    retriever=vectorstore.as_retriever(),
    return_source_documents=True
)

# 質問をして回答を取得
question = "NotebookLMはどのような用途で使えますか？"
result = qa_chain.invoke({"query": question})

print("\n--- AI Answer ---")
print(result["result"])

print("\n--- Source Documents ---")
for doc in result["source_documents"]:
    print(f"Source: {doc.metadata.get('source', 'N/A')}, Page: {doc.metadata.get('page', 'N/A')}")
    print(doc.page_content[:200]) # ソースドキュメントの冒頭を表示
    print("-" * 30)
```

### 5. その他のベクトルデータベース

- **FAISS:** `pip install faiss-cpu` でインストール。大量のベクトルをインメモリで高速に検索できます。ディスクに保存してロードする機能もあります。
    
    Python
    
    ```
    from langchain_community.vectorstores import FAISS
    # ... chunks と embeddings_model は上記と同様 ...
    faiss_vectorstore = FAISS.from_documents(chunks, embeddings_model)
    faiss_vectorstore.save_local("faiss_index") # インデックスを保存
    
    # ロードする場合
    # faiss_vectorstore = FAISS.load_local("faiss_index", embeddings_model, allow_dangerous_deserialization=True) # allow_dangerous_deserialization は注意して使用
    ```
    
- **Pinecone / Weaviate / Milvus / Qdrant:** これらのデータベースは、より大規模なデータセットやプロダクション環境での利用に適しています。それぞれ独自のAPIキーやセットアップが必要になります。LangChainやLlamaIndexは、これらのデータベースとも簡単に連携できるコネクタを提供しています。
    
    Python
    
    ```
    # Pinecone の例 (要 Pinecone アカウントと API キー)
    # pip install pinecone-client langchain_pinecone
    # from langchain_pinecone import PineconeVectorStore
    # index_name = "my-notebooklm-index"
    # pinecone_vectorstore = PineconeVectorStore.from_documents(
    #     documents=chunks, embedding=embeddings_model, index_name=index_name
    # )
    ```
    

### まとめ

Gemini 1.5 Flashを使ったベクトルデータベースの構築は、資料のロード、チャンク化、Geminiの埋め込みモデルによるベクトル化、そしてChromaDBなどのベクトルデータベースへの保存という手順で進めます。LangChainのようなフレームワークを使うことで、これらのプロセスを効率的に実装できます。

初期開発ではChromaDBやFAISSのようなローカルで手軽に試せるものから始め、データの規模やシステム要件に応じて、よりスケーラブルなクラウドベースのベクトルデータベースへの移行を検討するのが良いでしょう。

---
## **技術的キーワード一覧**
### **🧠 LLM & Embedding**

|**キーワード**|**概要**|
|---|---|
|**Gemini 1.5 Flash**|Google LLM ファミリーのテキスト／マルチモーダル対応モデル。埋め込み生成と推論が可能な最新バージョン（本文時点）。|
|**Gemini 2.5 Flash**|将来リリースが想定される後継モデル。API 仕様や利用フローは 1.5 Flash とほぼ同等と予想される。|
|**Google Generative AI SDK**|Python から Gemini API（生成・埋め込み）を呼び出す公式ライブラリ。google-generativeai パッケージで提供。|
|**GoogleGenerativeAIEmbeddings**|LangChain で Gemini の埋め込みモデルをラップするクラス。テキストを高次元ベクトルに変換する。|
|**ChatGoogleGenerativeAI**|Gemini LLM をチャット形式で呼び出す LangChain ラッパークラス。温度やモデル名を指定して初期化。|
|**Vector Embeddings**|文書やクエリを多次元ベクトルに数値化した表現。意味的類似度検索の基盤となる。|

---

### **🔍 ベクトルデータベース & ライブラリ**

|**キーワード**|**概要**|
|---|---|
|**ベクトルデータベース**|埋め込みベクトルを格納し、近傍探索 (k-NN) を高速に行う専用 DB。RAG の検索層を担う。|
|**ChromaDB**|軽量 OSS のローカル向けベクトル DB。Python だけで容易に導入でき、開発初期に適する。|
|**FAISS**|Meta (旧 Facebook) 提供の C++/Python ライブラリ。大規模ベクトルをインメモリで高速検索。|
|**Pinecone**|フルマネージド SaaS 型ベクトル DB。水平スケールと耐障害性が特徴。|
|**Weaviate**|OSS/クラウド両対応のグラフ構造 DB。GraphQL API でベクトル検索とメタデータ検索を統合。|
|**Milvus**|分散アーキテクチャを採る OSS ベクトル DB。数十億レコード規模でも高スループット。|
|**Qdrant**|Rust 製の高性能ベクトル DB。gRPC／REST API を提供し、オンプレとクラウド両対応。|

---

### **🛠️ RAG & 前処理**

|**キーワード**|**概要**|
|---|---|
|**RAG（Retrieval-Augmented Generation）**|検索で取得した関連文書を LLM プロンプトに組み込み、根拠付き回答を生成する手法。|
|**チャンク化 (Chunking)**|長文を適切なサイズに分割し、検索精度と LLM コンテキスト制限の両方を最適化する工程。|
|**RecursiveCharacterTextSplitter**|LangChain 付属のテキスト分割クラス。文字数と重複率を柔軟に指定可能。|
|**Similarity Search**|問い合わせベクトルと DB 内ベクトルの類似度を計算し、上位 k 件を取り出す検索操作。|
|**RetrievalQA**|LangChain のチェーン。リトリーバで文書を取得し、LLM に渡して QA を実行・根拠も返す。|

---

### **📚 データロード & ファイル処理**

|**キーワード**|**概要**|
|---|---|
|**unstructured**|さまざまなファイル形式を統一インターフェースでテキスト抽出できる Python ライブラリ。|
|**pypdf**|PDF からテキストとメタデータを抽出する純粋 Python 実装。|
|**TextLoader / PyPDFLoader / Docx2txtLoader**|LangChain に含まれるフォーマット別ローダー。各種ドキュメントを Document オブジェクトに変換。|
|**DirectoryLoader / UnstructuredFileLoader**|ディレクトリ全体を再帰的に走査してファイルをロードするユーティリティ。|

---

### **💻 開発フレームワーク**

|**キーワード**|**概要**|
|---|---|
|**Python**|本実装例で使用するプログラミング言語。豊富な AI／データ処理ライブラリが利用可能。|
|**LangChain**|RAG パイプラインを部品化・簡素化する OSS フレームワーク。埋め込み・検索・LLM 呼び出しを統合。|
|**LlamaIndex**|既存データソースをインデックス化し、LLM で検索／要約する軽量ライブラリ。LangChain と補完的。|

---

### **☁️ 環境・クラウド設定**

|**キーワード**|**概要**|
|---|---|
|**Google Cloud Project**|Gemini API 利用時に作成する GCP プロジェクト。課金と API 有効化を管理。|
|**Vertex AI API / Generative Language API**|Gemini 系 LLM・埋め込みを提供する GCP サービス。プロジェクトで有効化が必要。|
|**APIキー / サービスアカウントキー**|Gemini API 呼び出しに用いる認証情報。環境変数で安全に管理するのが一般的。|
|**dotenv (python-dotenv)**|.env ファイルから環境変数を読み込むユーティリティ。ローカル開発で鍵をハードコードしないために使用。|

---

### **🗃️ ストレージ操作ユーティリティ**

|**キーワード**|**概要**|
|---|---|
|**persist_directory / save_local / load_local**|ChromaDB や FAISS でインデックスをディスク永続化・再ロードする際の API／引数。|